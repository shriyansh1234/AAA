{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f236cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "import datetime\n",
    "import lxml\n",
    "import smtplib\n",
    "from faker import Faker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a3fe8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18296\\1290060838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwebdriver_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/path/to/chromedriver'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwebdriver_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.amazon.com/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Wait for user to interact with the browser or perform some actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "webdriver_path = '/path/to/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=webdriver_path)\n",
    "driver.get('https://www.amazon.com/')\n",
    "\n",
    "# Wait for user to interact with the browser or perform some actions\n",
    "# For example, you can instruct the user to navigate to a product page on Amazon manually\n",
    "\n",
    "# Get the current URL of the browser\n",
    "url = driver.current_url\n",
    "\n",
    "# Print the URL to the console or store it in your database\n",
    "print(\"Current URL:\", url)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b531b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot has been detected... retrying ... use new identity: Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 5.0; Trident/4.0) ko) Chrome/61.0.811.0 Safari/536.0 .0.878.0 Mobile/65M595 Safari/532.0 Bot bypassed\n"
     ]
    }
   ],
   "source": [
    "# Connect to Website and pull in data\n",
    "reviewlist = []\n",
    "def get_soup_retry(url):\n",
    "    fake = Faker()\n",
    "    uag_random = fake.user_agent()\n",
    "\n",
    "    header = {\n",
    "        'User-Agent': uag_random,\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "    isCaptcha = True\n",
    "    while isCaptcha:\n",
    "        page = requests.get(url, headers=header)\n",
    "        assert page.status_code == 200\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        if 'captcha' in str(soup):\n",
    "            uag_random = fake.user_agent()\n",
    "            print(f'\\rBot has been detected... retrying ... use new identity: {uag_random} ', end='', flush=True)\n",
    "            continue\n",
    "        else:\n",
    "            print('Bot bypassed')\n",
    "            return soup\n",
    "\n",
    "def extractReviews(url):\n",
    "    soup = get_soup_retry(url)\n",
    "\n",
    "    headers = {'authority': 'www.amazon.com',\n",
    "                'pragma': 'no-cache',\n",
    "                'cache-control': 'no-cache',\n",
    "                'dnt': '1',\n",
    "                'upgrade-insecure-requests': '1',\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\",\n",
    "                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-US;q=0.9,en;q=0.8',}\n",
    "    reviews = soup.findAll('div', {'data-hook':\"review\"})\n",
    "    title = ' '\n",
    "    r_title= ' '\n",
    "    rating = ' '\n",
    "    body = ' '\n",
    "    \n",
    "    # print(reviews)\n",
    "    for item in reviews:\n",
    "            try:\n",
    "                title = soup.title.text.replace(\"Amazon.com: Customer reviews: \", \"\").strip()\n",
    "\n",
    "            except AttributeError:\n",
    "                title = 'N'\n",
    "            try:\n",
    "                r_title = item.find('a', {'data-hook':\"review-title\"}).text.strip().split('\\n')\n",
    "                r_title = r_title[1]\n",
    "            except AttributeError:\n",
    "                r_title = 'N'\n",
    "            try:\n",
    "                rating = item.find('i', {'data-hook': 'review-star-rating'}).text.strip(\"('')\")\n",
    "                rating= rating[0:3]\n",
    "            except AttributeError:\n",
    "                rating = 'N'\n",
    "            try:\n",
    "                body = item.find('span', {'data-hook': 'review-body'}).text.strip() \n",
    "            except AttributeError:\n",
    "                body = 'N'\n",
    "            review = {\n",
    "                \"Product Name\" : title,\n",
    "                \"Review Header\": r_title,\n",
    "                \"Rating\": rating,\n",
    "                \"Review Body\" : body\n",
    "            }\n",
    "            reviewlist.append(review)\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.amazon.com/Kokuyo-Campus-Notebook-Semi-B5-NO-3CATNX5/dp/B002C4KL88/?_encoding=UTF8&pd_rd_w=RfW2q&content-id=amzn1.sym.5f7e0a27-49c0-47d3-80b2-fd9271d863ca%3Aamzn1.symc.e5c80209-769f-4ade-a325-2eaec14b8e0e&pf_rd_p=5f7e0a27-49c0-47d3-80b2-fd9271d863ca&pf_rd_r=4X5H702HEFJEY5RY2A9B&pd_rd_wg=LxQa0&pd_rd_r=43f615dc-3e3a-403b-89d2-ac483baf1929&ref_=pd_gw_ci_mcx_mr_hp_atf_m'\n",
    "    reviewUrl = url.replace(\"dp\", \"product-reviews\") \n",
    "    extractReviews(reviewUrl)\n",
    "    if reviewlist ==[]:\n",
    "        reviewUrl = url.replace(\"dp\", \"product-reviews\") \n",
    "        extractReviews(reviewUrl)\n",
    "    df = pd.DataFrame(reviewlist)\n",
    "    df.to_excel('output.xlsx', index=False)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55cf51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import excel as a data frame and turning it into a string\n",
    "df = pd.read_excel('output.xlsx')\n",
    "reviews = df['Review Body']\n",
    "reviews_string = ' '.join(map(str,reviews))\n",
    "#reviews_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed29def4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "for $10, you get five notebooks in purple, blue, green, golden yellow, and pink. each notebook contains 30 pages (=60 pages front and back) the quality of the paper is amazing, and there's a little bit of show-through. the cover helps tremendously in keeping the corners from folding in my backpack. it's hard to tell if the notebooks are damaged, but it's a great value for the money.\n"
     ]
    }
   ],
   "source": [
    "# using google T5 abstractive summarization\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# initialize the tokenizer model\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)\n",
    "\n",
    "# input data into the code below to tokenize it:\n",
    "inputs = tokenizer.encode(\"summarize: \" + reviews_string, return_tensors='pt', max_length=512, padding='max_length', \n",
    "                          truncation=True)\n",
    "\n",
    "# generate the summary by using the model.generate function on T5\n",
    "summary_ids = model.generate(inputs, max_length=400, min_length=100, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the generated summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1d5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de3d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b05c1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For $10, you get five notebooks in purple, blue, green, golden yellow, and pink, each containing 30 pages (=60 pages front and back). One might say they're expensive, given that you can easily buy a composition book for 50 cents during back-to-school sales, but I just adore how slim these notebooks are, and the quality of the paper is amazing. Except for the very first and last pages, the notebooks open to a relatively flat position, or at least flatter than a composition book can open up.I was initially hesitant to buy these books because of how little pages each notebook has, but I jumped for it when I realized I have trouble finishing a standard composition book. So maybe I'd recommend buying other items to be shipped with your notebooks?Since using this textbook in class, I've found that, for me, a chapter's worth of notes in my economics class takes up only one page (front and back). I'm thinking of buying the 6mm notebooks simply because I'd like the extra lines per page, but these are really good if you like writing with college-ruled paper. I love this notebook so much I recommend getting liquid ink roll ball pens with this it will go perfectly with these  notebooks Iâ€™m telling you that right now and this is a great notebook for language writing I highly recommend :) okay so right off the bat, these things are thin !! I love this paper ever since my best friend introduced Japanese Notebooks to me in Little Tokyo, Los Angeles. I have been using kokuyo campus notebooks for 2 years now and this school year u decided to go with the A size, it's so far good and paper quality is the same.\n"
     ]
    }
   ],
   "source": [
    "# using nltk to summarize extractive\n",
    "'''    \n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "    # tokenize the text:\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(reviews_string)\n",
    "\n",
    "    # create a frequency table to keep a score of each word:\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    # create a dictionary to keep the score of each sentence:\n",
    "    sentences = sent_tokenize(reviews_string)\n",
    "    sentenceValue = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "            for word, freq in freqTable.items():\n",
    "                if word in sentence.lower():\n",
    "                    if word in sentence.lower():\n",
    "                        if sentence in sentenceValue:\n",
    "                            sentenceValue[sentence] += freq\n",
    "                        else:\n",
    "                            sentenceValue[sentence] = freq\n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue:\n",
    "        sumValues += sentenceValue[sentence]\n",
    "\n",
    "    # define the average value from the original text as such:\n",
    "    average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "    #store the sentences into our summary:\n",
    "    summary = ''\n",
    "    for sentence in sentences:\n",
    "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.7 * average)):\n",
    "            summary += \" \" + sentence\n",
    "    print(summary)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e95b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair Project\n",
    "#import all the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from statistics import mode\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,Attention\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#read the dataset file\n",
    "df=pd.read_csv(\"Reviews.csv\",nrows=100000)\n",
    "#drop the duplicate and na values from the records\n",
    "df.drop_duplicates(subset=['Text'],inplace=True)\n",
    "df.dropna(axis=0,inplace=True)\n",
    "input_data = df.loc[:,'Text']\n",
    "target_data = df.loc[:,'Summary']\n",
    "target.replace('', np.nan, inplace=True)\n",
    "\n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_words=[]\n",
    "target_words=[]\n",
    "contractions= pickle.load(open(\"contractions.pkl\",\"rb\"))['contractions']\n",
    "#initialize stop words and LancasterStemmer\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stemm=LancasterStemmer()\n",
    "\n",
    "def clean(texts,src):\n",
    "  #remove the html tags\n",
    "  texts = BeautifulSoup(texts, \"lxml\").text\n",
    "  #tokenize the text into words \n",
    "  words=word_tokenize(texts.lower())\n",
    "  #filter words which contains \\ \n",
    "  #integers or their length is less than or equal to 3\n",
    "  words= list(filter(lambda w:(w.isalpha() and len(w)>=3),words))\n",
    "  #contraction file to expand shortened words\n",
    "  words= [contractions[w] if w in contractions else w for w in words ]\n",
    "  #stem the words to their root word and filter stop words\n",
    "  if src==\"inputs\":\n",
    "    words= [stemm.stem(w) for w in words if w not in stop_words]\n",
    "  else:\n",
    "    words= [w for w in words if w not in stop_words]\n",
    "  return words\n",
    "\n",
    "#pass the input records and taret records\n",
    "for in_txt,tr_txt in zip(input_data,target_data):\n",
    "  in_words= clean(in_txt,\"inputs\")\n",
    "  input_texts+= [' '.join(in_words)]\n",
    "  input_words+= in_words\n",
    "  #add 'sos' at start and 'eos' at end of text\n",
    "  tr_words= clean(\"sos \"+tr_txt+\" eos\",\"target\")\n",
    "  target_texts+= [' '.join(tr_words)]\n",
    "  target_words+= tr_words\n",
    "\n",
    "#store only unique words from input and target list of words\n",
    "input_words = sorted(list(set(input_words)))\n",
    "target_words = sorted(list(set(target_words)))\n",
    "num_in_words = len(input_words) #total number of input words\n",
    "num_tr_words = len(target_words) #total number of target words\n",
    "\n",
    "#get the length of the input and target texts which appears most often  \n",
    "max_in_len = mode([len(i) for i in input_texts])\n",
    "max_tr_len = mode([len(i) for i in target_texts])\n",
    "\n",
    "print(\"number of input words : \",num_in_words)\n",
    "print(\"number of target words : \",num_tr_words)\n",
    "print(\"maximum input length : \",max_in_len)\n",
    "print(\"maximum target length : \",max_tr_len)\n",
    "\n",
    "\n",
    "#split the input and target text into 80:20 ratio or testing size of 20%.\n",
    "x_train,x_test,y_train,y_test=train_test_split(input_texts,target_texts,test_size=0.2,random_state=0)\n",
    "\n",
    "#train the tokenizer with all the words\n",
    "in_tokenizer = Tokenizer()\n",
    "in_tokenizer.fit_on_texts(x_train)\n",
    "tr_tokenizer = Tokenizer()\n",
    "tr_tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "#convert text into sequence of integers\n",
    "#where the integer will be the index of that word\n",
    "x_train= in_tokenizer.texts_to_sequences(x_train) \n",
    "y_train= tr_tokenizer.texts_to_sequences(y_train) \n",
    "\n",
    "#pad array of 0's if the length is less than the maximum length \n",
    "en_in_data= pad_sequences(x_train,  maxlen=max_in_len, padding='post') \n",
    "dec_data= pad_sequences(y_train,  maxlen=max_tr_len, padding='post')\n",
    "\n",
    "#decoder input data will not include the last word \n",
    "#i.e. 'eos' in decoder input data\n",
    "dec_in_data = dec_data[:,:-1]\n",
    "#decoder target data will be one time step ahead as it will not include\n",
    "# the first word i.e 'sos'\n",
    "dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]\n",
    "\n",
    "K.clear_session() \n",
    "latent_dim = 500\n",
    "\n",
    "#create input object of total number of input words\n",
    "en_inputs = Input(shape=(max_in_len,)) \n",
    "en_embedding = Embedding(num_in_words+1, latent_dim)(en_inputs) \n",
    "\n",
    "#create 3 stacked LSTM layer with the shape of hidden dimension\n",
    "#LSTM 1\n",
    "en_lstm1= LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "en_outputs1, state_h1, state_c1= en_lstm1(en_embedding) \n",
    "\n",
    "#LSTM2\n",
    "en_lstm2= LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "en_outputs2, state_h2, state_c2= en_lstm2(en_outputs1) \n",
    "\n",
    "#LSTM3\n",
    "en_lstm3= LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "en_outputs3 , state_h3 , state_c3= en_lstm3(en_outputs2)\n",
    "\n",
    "#encoder states\n",
    "en_states= [state_h3, state_c3]\n",
    "\n",
    "# Decoder. \n",
    "dec_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(num_tr_words+1, latent_dim) \n",
    "dec_embedding = dec_emb_layer(dec_inputs) \n",
    "\n",
    "#initialize decoder's LSTM layer with the output states of encoder\n",
    "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "dec_outputs, *_ = dec_lstm(dec_embedding,initial_state=en_states) \n",
    "\n",
    "#Attention layer\n",
    "attention =Attention()\n",
    "attn_out = attention([dec_outputs,en_outputs3])\n",
    "\n",
    "#Concatenate the attention output with the decoder ouputs\n",
    "merge=Concatenate(axis=-1, name='concat_layer1')([dec_outputs,attn_out])\n",
    "\n",
    "#Dense layer (output layer)\n",
    "dec_dense = Dense(num_tr_words+1, activation='softmax') \n",
    "dec_outputs = dec_dense(merge) \n",
    "\n",
    "#Mode class and model summary\n",
    "model = Model([en_inputs, dec_inputs], dec_outputs) \n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "model.compile( \n",
    "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] ) \n",
    "model.fit( \n",
    "    [en_in_data, dec_in_data],\n",
    "    dec_tr_data, \n",
    "    batch_size=512, \n",
    "    epochs=10, \n",
    "    validation_split=0.1,\n",
    "    )\n",
    "\n",
    "#Save model\n",
    "model.save(\"s2s\")\n",
    "\n",
    "# encoder inference\n",
    "latent_dim=500\n",
    "#load the model\n",
    "model = models.load_model(\"s2s\")\n",
    "\n",
    "#construct encoder model from the output of 6 layer i.e.last LSTM layer\n",
    "en_outputs,state_h_enc,state_c_enc = model.layers[6].output\n",
    "en_states=[state_h_enc,state_c_enc]\n",
    "#add input and state from the layer.\n",
    "en_model = Model(model.input[0],[en_outputs]+en_states)\n",
    "\n",
    "# decoder inference\n",
    "#create Input object for hidden and cell state for decoder\n",
    "#shape of layer with hidden or latent dimension\n",
    "dec_state_input_h = Input(shape=(latent_dim,))\n",
    "dec_state_input_c = Input(shape=(latent_dim,))\n",
    "dec_hidden_state_input = Input(shape=(max_in_len,latent_dim))\n",
    "\n",
    "# Get the embeddings and input layer from the model\n",
    "dec_inputs = model.input[1]\n",
    "dec_emb_layer = model.layers[5]\n",
    "dec_lstm = model.layers[7]\n",
    "dec_embedding= dec_emb_layer(dec_inputs)\n",
    "\n",
    "#add input and initialize LSTM layer with encoder LSTM states.\n",
    "dec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, initial_state=[dec_state_input_h,dec_state_input_c])\n",
    "\n",
    "#Attention layer\n",
    "attention = model.layers[8]\n",
    "attn_out2 = attention([dec_outputs2,dec_hidden_state_input])\n",
    "\n",
    "merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out2])\n",
    "\n",
    "#Dense layer\n",
    "dec_dense = model.layers[10]\n",
    "dec_outputs2 = dec_dense(merge2)\n",
    "\n",
    "# Finally define the Model Class\n",
    "dec_model = Model(\n",
    "[dec_inputs] + [dec_hidden_state_input,dec_state_input_h,dec_state_input_c],\n",
    "[dec_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "#create a dictionary with a key as index and value as words.\n",
    "reverse_target_word_index = tr_tokenizer.index_word\n",
    "reverse_source_word_index = in_tokenizer.index_word\n",
    "target_word_index = tr_tokenizer.word_index\n",
    "reverse_target_word_index[0]=' '\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    #get the encoder output and states by passing the input sequence\n",
    "    en_out, en_h, en_c= en_model.predict(input_seq)\n",
    "\n",
    "    #target sequence with inital word as 'sos'\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_word_index['sos']\n",
    "\n",
    "    #if the iteration reaches the end of text than it will be stop the iteration\n",
    "    stop_condition = False\n",
    "    #append every predicted word in decoded sentence\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: \n",
    "        #get predicted output, hidden and cell state.\n",
    "        output_words, dec_h, dec_c= dec_model.predict([target_seq] + [en_out,en_h, en_c])\n",
    "        \n",
    "        #get the index and from the dictionary get the word for that index.\n",
    "        word_index = np.argmax(output_words[0, -1, :])\n",
    "        text_word = reverse_target_word_index[word_index]\n",
    "        decoded_sentence += text_word +\" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find a stop word or last word.\n",
    "        if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n",
    "          stop_condition = True\n",
    "        \n",
    "        #update target sequence to the current word index.\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = word_index\n",
    "        en_h, en_c = dec_h, dec_c\n",
    "    \n",
    "    #return the deocded sentence\n",
    "    return decoded_sentence\n",
    "\n",
    "inp_review = input(\"Enter : \")\n",
    "print(\"Review :\",inp_review)\n",
    "\n",
    "inp_review = clean(inp_review,\"inputs\")\n",
    "inp_review = ' '.join(inp_review)\n",
    "inp_x= in_tokenizer.texts_to_sequences([inp_review]) \n",
    "inp_x= pad_sequences(inp_x,  maxlen=max_in_len, padding='post')\n",
    "\n",
    "summary=decode_sequence(inp_x.reshape(1,max_in_len))\n",
    "if 'eos' in summary :\n",
    "  summary=summary.replace('eos','')\n",
    "print(\"\\nPredicted summary:\",summary);print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f2c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks historical price and suggests a month or day to buy the product at its lowest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If uou want to try sending yourself an email (just for fun) when a price hits below a certain level you can try it\n",
    "# out with this script\n",
    "\n",
    "def send_mail():\n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
    "    server.ehlo()\n",
    "    #server.starttls()\n",
    "    server.ehlo()\n",
    "    server.login('AlexTheAnalyst95@gmail.com','xxxxxxxxxxxxxx')\n",
    "    \n",
    "    subject = \"The Shirt you want is below $15! Now is your chance to buy!\"\n",
    "    body = \"Alex, This is the moment we have been waiting for. Now is your chance to pick up the shirt of your dreams. Don't mess it up! Link here: https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_3?dchild=1&keywords=data+analyst+tshirt&qid=1626655184&sr=8-3\"\n",
    "   \n",
    "    msg = f\"Subject: {subject}\\n\\n{body}\"\n",
    "    \n",
    "    server.sendmail(\n",
    "        'AlexTheAnalyst95@gmail.com',\n",
    "        msg\n",
    "     \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
