{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f236cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "import datetime\n",
    "import lxml\n",
    "import smtplib\n",
    "from faker import Faker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a3fe8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18296\\1290060838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwebdriver_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/path/to/chromedriver'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwebdriver_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.amazon.com/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Wait for user to interact with the browser or perform some actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "webdriver_path = '/path/to/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=webdriver_path)\n",
    "driver.get('https://www.amazon.com/')\n",
    "\n",
    "# Wait for user to interact with the browser or perform some actions\n",
    "# For example, you can instruct the user to navigate to a product page on Amazon manually\n",
    "\n",
    "# Get the current URL of the browser\n",
    "url = driver.current_url\n",
    "\n",
    "# Print the URL to the console or store it in your database\n",
    "print(\"Current URL:\", url)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b531b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot has been detected... retrying ... use new identity: Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 5.0; Trident/4.0) ko) Chrome/61.0.811.0 Safari/536.0 .0.878.0 Mobile/65M595 Safari/532.0 Bot bypassed\n"
     ]
    }
   ],
   "source": [
    "# Connect to Website and pull in data\n",
    "reviewlist = []\n",
    "def get_soup_retry(url):\n",
    "    fake = Faker()\n",
    "    uag_random = fake.user_agent()\n",
    "\n",
    "    header = {\n",
    "        'User-Agent': uag_random,\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "    isCaptcha = True\n",
    "    while isCaptcha:\n",
    "        page = requests.get(url, headers=header)\n",
    "        assert page.status_code == 200\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        if 'captcha' in str(soup):\n",
    "            uag_random = fake.user_agent()\n",
    "            print(f'\\rBot has been detected... retrying ... use new identity: {uag_random} ', end='', flush=True)\n",
    "            continue\n",
    "        else:\n",
    "            print('Bot bypassed')\n",
    "            return soup\n",
    "\n",
    "def extractReviews(url):\n",
    "    soup = get_soup_retry(url)\n",
    "\n",
    "    headers = {'authority': 'www.amazon.com',\n",
    "                'pragma': 'no-cache',\n",
    "                'cache-control': 'no-cache',\n",
    "                'dnt': '1',\n",
    "                'upgrade-insecure-requests': '1',\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\",\n",
    "                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-US;q=0.9,en;q=0.8',}\n",
    "    reviews = soup.findAll('div', {'data-hook':\"review\"})\n",
    "    title = ' '\n",
    "    r_title= ' '\n",
    "    rating = ' '\n",
    "    body = ' '\n",
    "    \n",
    "    # print(reviews)\n",
    "    for item in reviews:\n",
    "            try:\n",
    "                title = soup.title.text.replace(\"Amazon.com: Customer reviews: \", \"\").strip()\n",
    "\n",
    "            except AttributeError:\n",
    "                title = 'N'\n",
    "            try:\n",
    "                r_title = item.find('a', {'data-hook':\"review-title\"}).text.strip().split('\\n')\n",
    "                r_title = r_title[1]\n",
    "            except AttributeError:\n",
    "                r_title = 'N'\n",
    "            try:\n",
    "                rating = item.find('i', {'data-hook': 'review-star-rating'}).text.strip(\"('')\")\n",
    "                rating= rating[0:3]\n",
    "            except AttributeError:\n",
    "                rating = 'N'\n",
    "            try:\n",
    "                body = item.find('span', {'data-hook': 'review-body'}).text.strip() \n",
    "            except AttributeError:\n",
    "                body = 'N'\n",
    "            review = {\n",
    "                \"Product Name\" : title,\n",
    "                \"Review Header\": r_title,\n",
    "                \"Rating\": rating,\n",
    "                \"Review Body\" : body\n",
    "            }\n",
    "            reviewlist.append(review)\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.amazon.com/Kokuyo-Campus-Notebook-Semi-B5-NO-3CATNX5/dp/B002C4KL88/?_encoding=UTF8&pd_rd_w=RfW2q&content-id=amzn1.sym.5f7e0a27-49c0-47d3-80b2-fd9271d863ca%3Aamzn1.symc.e5c80209-769f-4ade-a325-2eaec14b8e0e&pf_rd_p=5f7e0a27-49c0-47d3-80b2-fd9271d863ca&pf_rd_r=4X5H702HEFJEY5RY2A9B&pd_rd_wg=LxQa0&pd_rd_r=43f615dc-3e3a-403b-89d2-ac483baf1929&ref_=pd_gw_ci_mcx_mr_hp_atf_m'\n",
    "    reviewUrl = url.replace(\"dp\", \"product-reviews\") \n",
    "    extractReviews(reviewUrl)\n",
    "    if reviewlist ==[]:\n",
    "        reviewUrl = url.replace(\"dp\", \"product-reviews\") \n",
    "        extractReviews(reviewUrl)\n",
    "    df = pd.DataFrame(reviewlist)\n",
    "    df.to_excel('output.xlsx', index=False)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55cf51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import excel as a data frame and turning it into a string\n",
    "df = pd.read_excel('output.xlsx')\n",
    "reviews = df['Review Body']\n",
    "reviews_string = ' '.join(map(str,reviews))\n",
    "#reviews_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed29def4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "for $10, you get five notebooks in purple, blue, green, golden yellow, and pink. each notebook contains 30 pages (=60 pages front and back) the quality of the paper is amazing, and there's a little bit of show-through. the cover helps tremendously in keeping the corners from folding in my backpack. it's hard to tell if the notebooks are damaged, but it's a great value for the money.\n"
     ]
    }
   ],
   "source": [
    "# using google T5 abstractive summarization\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# initialize the tokenizer model\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)\n",
    "\n",
    "# input data into the code below to tokenize it:\n",
    "inputs = tokenizer.encode(\"summarize: \" + reviews_string, return_tensors='pt', max_length=512, padding='max_length', \n",
    "                          truncation=True)\n",
    "\n",
    "# generate the summary by using the model.generate function on T5\n",
    "summary_ids = model.generate(inputs, max_length=400, min_length=100, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the generated summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1d5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de3d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b05c1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For $10, you get five notebooks in purple, blue, green, golden yellow, and pink, each containing 30 pages (=60 pages front and back). One might say they're expensive, given that you can easily buy a composition book for 50 cents during back-to-school sales, but I just adore how slim these notebooks are, and the quality of the paper is amazing. Except for the very first and last pages, the notebooks open to a relatively flat position, or at least flatter than a composition book can open up.I was initially hesitant to buy these books because of how little pages each notebook has, but I jumped for it when I realized I have trouble finishing a standard composition book. So maybe I'd recommend buying other items to be shipped with your notebooks?Since using this textbook in class, I've found that, for me, a chapter's worth of notes in my economics class takes up only one page (front and back). I'm thinking of buying the 6mm notebooks simply because I'd like the extra lines per page, but these are really good if you like writing with college-ruled paper. I love this notebook so much I recommend getting liquid ink roll ball pens with this it will go perfectly with these  notebooks Iâ€™m telling you that right now and this is a great notebook for language writing I highly recommend :) okay so right off the bat, these things are thin !! I love this paper ever since my best friend introduced Japanese Notebooks to me in Little Tokyo, Los Angeles. I have been using kokuyo campus notebooks for 2 years now and this school year u decided to go with the A size, it's so far good and paper quality is the same.\n"
     ]
    }
   ],
   "source": [
    "# using nltk to summarize extractive\n",
    "'''    \n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "    # tokenize the text:\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(reviews_string)\n",
    "\n",
    "    # create a frequency table to keep a score of each word:\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    # create a dictionary to keep the score of each sentence:\n",
    "    sentences = sent_tokenize(reviews_string)\n",
    "    sentenceValue = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "            for word, freq in freqTable.items():\n",
    "                if word in sentence.lower():\n",
    "                    if word in sentence.lower():\n",
    "                        if sentence in sentenceValue:\n",
    "                            sentenceValue[sentence] += freq\n",
    "                        else:\n",
    "                            sentenceValue[sentence] = freq\n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue:\n",
    "        sumValues += sentenceValue[sentence]\n",
    "\n",
    "    # define the average value from the original text as such:\n",
    "    average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "    #store the sentences into our summary:\n",
    "    summary = ''\n",
    "    for sentence in sentences:\n",
    "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.7 * average)):\n",
    "            summary += \" \" + sentence\n",
    "    print(summary)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e95b9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f2c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks historical price and suggests a month or day to buy the product at its lowest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If uou want to try sending yourself an email (just for fun) when a price hits below a certain level you can try it\n",
    "# out with this script\n",
    "\n",
    "def send_mail():\n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
    "    server.ehlo()\n",
    "    #server.starttls()\n",
    "    server.ehlo()\n",
    "    server.login('AlexTheAnalyst95@gmail.com','xxxxxxxxxxxxxx')\n",
    "    \n",
    "    subject = \"The Shirt you want is below $15! Now is your chance to buy!\"\n",
    "    body = \"Alex, This is the moment we have been waiting for. Now is your chance to pick up the shirt of your dreams. Don't mess it up! Link here: https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_3?dchild=1&keywords=data+analyst+tshirt&qid=1626655184&sr=8-3\"\n",
    "   \n",
    "    msg = f\"Subject: {subject}\\n\\n{body}\"\n",
    "    \n",
    "    server.sendmail(\n",
    "        'AlexTheAnalyst95@gmail.com',\n",
    "        msg\n",
    "     \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
